# UI.py
import sys
import os
import cv2
import numpy as np
import speech_recognition as sr
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,
    QPushButton, QTextEdit, QLabel, QFileDialog, QMessageBox
)
from PyQt5.QtCore import Qt, QThread, pyqtSignal
from PyQt5.QtGui import QTextCursor, QImage, QPixmap
import torch
from torchvision import transforms
from PIL import Image
from CNN_LSTM import FeatureExtractor, MultiModalCNNTransformerModel
import mediapipe as mp

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class SpeechThread(QThread):
    result_signal = pyqtSignal(str)

    def run(self):
        r = sr.Recognizer()
        try:
            with sr.Microphone() as source:
                r.adjust_for_ambient_noise(source, duration=1)
                audio = r.listen(source, timeout=5, phrase_time_limit=8)
                try:
                    text = r.recognize_google(audio, language='zh-CN')
                    self.result_signal.emit(f"[è¯­éŸ³] {text}")
                except sr.UnknownValueError:
                    self.result_signal.emit("[è¯­éŸ³] æ— æ³•è¯†åˆ«è¯­éŸ³")
                except sr.RequestError:
                    self.result_signal.emit("[è¯­éŸ³] æœåŠ¡ä¸å¯ç”¨")
        except Exception as e:
            self.result_signal.emit("[è¯­éŸ³] éº¦å…‹é£è®¿é—®å¤±è´¥")

class CameraThread(QThread):
    result_signal = pyqtSignal(str)
    frame_signal = pyqtSignal(QImage)
    result_signal = pyqtSignal(str, float)  # æ–°å¢floatå‚æ•°

    def __init__(self, model, feature_extractor, label_map):
        super().__init__()
        self.model = model
        self.feature_extractor = feature_extractor
        self.label_map = label_map
        self.running = True
        self.frame_buffer = []
        self.keypoint_buffer = []
        self.max_sequence_length = 170
        self.predict_interval = 170
        self.frame_counter = 0
        self.mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=2)

    def run(self):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            self.result_signal.emit("[é”™è¯¯] æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return

        try:
            while self.running:
                ret, frame = cap.read()
                if not ret:
                    self.result_signal.emit("[é”™è¯¯] è§†é¢‘æµä¸­æ–­")
                    break

                # æ˜¾ç¤ºå®æ—¶ç”»é¢
                rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                h, w, ch = rgb_image.shape
                bytes_per_line = ch * w
                qt_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
                self.frame_signal.emit(qt_image.scaled(640, 480, Qt.KeepAspectRatio))

                # å¤„ç†å¸§å’Œå…³é”®ç‚¹
                processed_frame, keypoints = self._process_frame(frame)
                self.frame_buffer.append(processed_frame)
                self.keypoint_buffer.append(keypoints)

                # ä¿æŒå›ºå®šåºåˆ—é•¿åº¦
                if len(self.frame_buffer) > self.max_sequence_length:
                    self.frame_buffer = self.frame_buffer[-self.max_sequence_length:]
                    self.keypoint_buffer = self.keypoint_buffer[-self.max_sequence_length:]
                
                # å®šæœŸé¢„æµ‹
                self.frame_counter += 1
                if self.frame_counter % self.predict_interval == 0:
                    pred_label, confidence = self._predict()
                    self.result_signal.emit(pred_label, confidence)

        finally:
            cap.release()
            cv2.destroyAllWindows()
            self.mp_hands.close()

    def _process_frame(self, frame):
        """å¤„ç†å•å¸§å¹¶æå–å…³é”®ç‚¹"""
        # é¢„å¤„ç†è§†è§‰å¸§
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)
        processed_tensor = transform(pil_image)

        # æå–å…³é”®ç‚¹
        hands_results = self.mp_hands.process(frame_rgb)
        keypoints = np.zeros(126, dtype=np.float32)
        if hands_results.multi_hand_landmarks:
            hands = hands_results.multi_hand_landmarks[:2]
            for hand_idx, hand in enumerate(hands):
                start = hand_idx * 63
                for lm_idx, landmark in enumerate(hand.landmark[:21]):
                    pos = start + lm_idx * 3
                    if pos + 2 < 126:
                        keypoints[pos] = landmark.x
                        keypoints[pos+1] = landmark.y
                        keypoints[pos+2] = landmark.z

        return processed_tensor, torch.tensor(keypoints, dtype=torch.float32)

    def _predict(self):
        """æ‰§è¡Œå¤šæ¨¡æ€é¢„æµ‹"""
        if len(self.frame_buffer) < 10:
            return "ç­‰å¾…æ›´å¤šå¸§..."

        # è½¬æ¢ä¸ºTensor
        frames_tensor = torch.stack(self.frame_buffer).to(device)  # [seq_len, C, H, W]
        keypoints_tensor = torch.stack(self.keypoint_buffer).to(device)  # [seq_len, 126]

        with torch.no_grad():
            # æå–è§†è§‰ç‰¹å¾
            visual_features = self.feature_extractor(
                frames_tensor.view(-1, 3, 256, 256)
            ).view(1, -1, 512)  # [1, seq_len, 512]

            # æ‹¼æ¥å…³é”®ç‚¹ç‰¹å¾
            combined_features = torch.cat([
                visual_features,
                keypoints_tensor.unsqueeze(0)
            ], dim=2)  # [1, seq_len, 638]

            # è°ƒæ•´åºåˆ—é•¿åº¦
            seq_len = combined_features.shape[1]
            if seq_len < self.max_sequence_length:
                padding = torch.zeros(1, self.max_sequence_length-seq_len, 638).to(device)
                combined_features = torch.cat([combined_features, padding], dim=1)
            else:
                combined_features = combined_features[:, :self.max_sequence_length]

            outputs = self.model(combined_features)
            probs = torch.softmax(outputs, dim=1)
            

        pred_idx = torch.argmax(probs).item()
        confidence = probs[0][pred_idx].item() 
        return self.label_map.get(pred_idx, "æœªçŸ¥æ ‡ç­¾"), confidence

    def stop(self):
        self.running = False

class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.feature_extractor = None
        self.model = None
        self.preview_label = QLabel()
        self.camera_thread = None
        self.label_map = {}
        self.initUI()
        self.load_models()
    
    def append_history_with_confidence(self, label, confidence):
        """æ·»åŠ å¸¦ç½®ä¿¡åº¦çš„å†å²è®°å½•"""
        from datetime import datetime
        timestamp = datetime.now().strftime("%H:%M:%S")
        text = f"[{timestamp}] [å®æ—¶] {label} (ç½®ä¿¡åº¦: {confidence:.2%})"
        
        cursor = self.history.textCursor()
        cursor.movePosition(QTextCursor.End)
        cursor.insertText(text + "\n")
        self.history.ensureCursorVisible()

    def initUI(self):
        """åˆå§‹åŒ–ç”¨æˆ·ç•Œé¢"""
        self.setWindowTitle("æ‰‹è¯­è¯†åˆ«ç³»ç»Ÿ v3.0")
        self.setGeometry(100, 100, 1024, 768)

        # ä¸»å¸ƒå±€
        main_widget = QWidget()
        main_layout = QHBoxLayout()
        
        # å·¦ä¾§æ§åˆ¶é¢æ¿
        control_panel = QVBoxLayout()
        control_panel.addWidget(QLabel("æ“ä½œé€‰é¡¹ï¼š", styleSheet="font-weight: bold;"))
        
        # åŠŸèƒ½æŒ‰é’®
        self.btn_video = QPushButton("ğŸ“ å¯¼å…¥è§†é¢‘æ–‡ä»¶")
        self.btn_image = QPushButton("ğŸ“· å¯¼å…¥å›¾ç‰‡åºåˆ—")
        self.btn_camera = QPushButton("ğŸ¥ å¼€å¯å®æ—¶è¯†åˆ«")
        self.btn_voice = QPushButton("ğŸ¤ è¯­éŸ³è¾“å…¥")
        
        # æŒ‰é’®æ ·å¼
        button_style = """
            QPushButton { 
                padding: 10px; 
                font-size: 14px; 
                min-width: 150px;
                border-radius: 5px;
                margin: 5px;
            }
            QPushButton:hover { background-color: #f0f0f0; }
        """
        for btn in [self.btn_video, self.btn_image, self.btn_camera, self.btn_voice]:
            btn.setStyleSheet(button_style)
        
        # ç»‘å®šäº‹ä»¶
        self.btn_video.clicked.connect(self.select_video)
        self.btn_image.clicked.connect(self.select_images)
        self.btn_camera.clicked.connect(self.toggle_camera)
        self.btn_voice.clicked.connect(self.speech_input)

        # ä¿¡æ¯æç¤º
        self.lbl_status = QLabel("å°±ç»ª")
        self.lbl_status.setStyleSheet("color: #666; font-size: 12px; padding: 5px;")
        
        # å†å²è®°å½•
        self.history = QTextEdit()
        self.history.setStyleSheet("""
            font-family: 'Microsoft YaHei'; 
            font-size: 12px; 
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 5px;
        """)
        self.history.setPlaceholderText("è¯†åˆ«æ—¥å¿—å°†æ˜¾ç¤ºåœ¨æ­¤å¤„...")

        # å³ä¾§é¢„è§ˆé¢æ¿
        preview_panel = QVBoxLayout()
        preview_panel.addWidget(QLabel("å®æ—¶é¢„è§ˆï¼š", styleSheet="font-weight: bold;"))
        self.preview_label.setAlignment(Qt.AlignCenter)
        self.preview_label.setMinimumSize(640, 480)
        self.preview_label.setStyleSheet("""
            background: #333; 
            border-radius: 10px;
            border: 2px solid #666;
        """)
        preview_panel.addWidget(self.preview_label)

        # ç»„è£…ç•Œé¢
        control_panel.addWidget(self.btn_video)
        control_panel.addWidget(self.btn_image)
        control_panel.addWidget(self.btn_camera)
        control_panel.addWidget(self.btn_voice)
        control_panel.addWidget(self.lbl_status)
        control_panel.addWidget(QLabel("æ“ä½œè®°å½•ï¼š", styleSheet="font-weight: bold;"))
        control_panel.addWidget(self.history)

        main_layout.addLayout(control_panel, 35)
        main_layout.addLayout(preview_panel, 65)
        main_widget.setLayout(main_layout)
        self.setCentralWidget(main_widget)
        self.btn_test = QPushButton("ğŸ”§ è¿è¡Œè¯Šæ–­æµ‹è¯•")
        self.btn_test.setStyleSheet("background-color: #ff9900;")
        self.btn_test.clicked.connect(self.run_diagnostic_test)
        control_panel.addWidget(self.btn_test)

    def load_models(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            model_path = r'D:\ctcn_2\models\best_model.pth'
            checkpoint = torch.load(model_path, map_location=device)
            
            # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
            self.feature_extractor = FeatureExtractor(model_name='resnet18').to(device)
            self.feature_extractor.eval()
            
            # åˆå§‹åŒ–Transformeræ¨¡å‹
            self.model = MultiModalCNNTransformerModel(
                feature_dim=638,
                num_classes=len(checkpoint['index_to_label']),
                heads=2
            ).to(device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            
            # åŠ è½½æ ‡ç­¾æ˜ å°„
            self.label_map = {int(k): v for k, v in checkpoint['index_to_label'].items()}

        except Exception as e:
            QMessageBox.critical(self, "é”™è¯¯", f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            sys.exit(1)

    def toggle_camera(self):
        """åˆ‡æ¢æ‘„åƒå¤´çŠ¶æ€"""
        if self.camera_thread and self.camera_thread.isRunning():
            self.camera_thread.stop()
            self.camera_thread.quit()
            self.camera_thread.wait()
            self.btn_camera.setText("ğŸ¥ å¼€å¯å®æ—¶è¯†åˆ«")
            self.lbl_status.setText("å·²åœæ­¢æ‘„åƒå¤´è¯†åˆ«")
        else:
            self.start_camera()
            self.btn_camera.setText("â¹ï¸ åœæ­¢è¯†åˆ«")
            self.lbl_status.setText("å®æ—¶è¯†åˆ«è¿è¡Œä¸­...")

    def start_camera(self):
        """å¯åŠ¨æ‘„åƒå¤´çº¿ç¨‹"""
        self.camera_thread = CameraThread(
            model=self.model,
            feature_extractor=self.feature_extractor,
            label_map=self.label_map
        )
        # ç»‘å®šæ–°ä¿¡å·ï¼ˆå¸¦ç½®ä¿¡åº¦ï¼‰
        self.camera_thread.frame_signal.connect(self.update_preview)
        self.camera_thread.result_signal.connect(self.append_history_with_confidence)  # æ­£ç¡®ä½ç½®
        self.camera_thread.start()

    def select_video(self):
        """é€‰æ‹©è§†é¢‘æ–‡ä»¶"""
        try:
            path, _ = QFileDialog.getOpenFileName(
                self, 
                "é€‰æ‹©è§†é¢‘æ–‡ä»¶", 
                "", 
                "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi);;æ‰€æœ‰æ–‡ä»¶ (*)"
            )
            if path:
                if not os.path.exists(path):
                    raise FileNotFoundError("æ–‡ä»¶ä¸å­˜åœ¨")

                # æ‰§è¡Œé¢„æµ‹
                result = self._predict_media(path, is_video=True)
                self.append_history(f"[è§†é¢‘è¯†åˆ«] ç»“æœ: {result}")

        except Exception as e:
            self.show_error("è§†é¢‘å¤„ç†é”™è¯¯", str(e))

    def select_images(self):
        """é€‰æ‹©å›¾ç‰‡åºåˆ—"""
        try:
            path = QFileDialog.getExistingDirectory(self, "é€‰æ‹©å›¾ç‰‡æ–‡ä»¶å¤¹")
            if path:
                result = self._predict_media(path, is_video=False)
                self.append_history(f"[å›¾ç‰‡è¯†åˆ«] ç»“æœ: {result}")

        except Exception as e:
            self.show_error("å›¾ç‰‡å¤„ç†é”™è¯¯", str(e))

    def _predict_media(self, input_path, is_video):
        """ç»Ÿä¸€åª’ä½“é¢„æµ‹æ–¹æ³•"""
        mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=2)
    
        frames = []
        keypoints_list = []
        
        try:
            if is_video:
                cap = cv2.VideoCapture(input_path)
                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break
                    processed_frame, keypoints = self._process_frame_with_keypoints(frame, mp_hands)
                    frames.append(processed_frame)
                    keypoints_list.append(keypoints)
                cap.release()
            else:
                valid_exts = ('.png', '.jpg', '.jpeg')
                for fname in sorted(os.listdir(input_path)):
                    if fname.lower().endswith(valid_exts):
                        frame = cv2.imread(os.path.join(input_path, fname))
                        if frame is not None:
                            processed_frame, keypoints = self._process_frame_with_keypoints(frame, mp_hands)
                            frames.append(processed_frame)
                            keypoints_list.append(keypoints)
            
            if not frames:
                return "æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¾“å…¥"

            # è½¬æ¢ä¸ºTensorå¹¶å¤„ç†
            frames_tensor = torch.stack(frames).to(device)
            keypoints_tensor = torch.stack(keypoints_list).to(device)

            with torch.no_grad():
                # æå–å¹¶æ‹¼æ¥ç‰¹å¾
                visual_features = self.feature_extractor(
                    frames_tensor.view(-1, 3, 256, 256)
                ).view(1, -1, 512)
                combined_features = torch.cat([
                    visual_features,
                    keypoints_tensor.unsqueeze(0)
                ], dim=2)

                # è°ƒæ•´åºåˆ—é•¿åº¦
                seq_len = combined_features.shape[1]
                if seq_len < 170:
                    padding = torch.zeros(1, 170-seq_len, 638).to(device)
                    combined_features = torch.cat([combined_features, padding], dim=1)
                else:
                    combined_features = combined_features[:, :170]

                outputs = self.model(combined_features)
                probs = torch.softmax(outputs, dim=1)
            
            # å¤„ç†é¢„æµ‹ç»“æœ
            pred_idx = torch.argmax(probs).item()
            label = self.label_map.get(pred_idx, "æœªçŸ¥æ ‡ç­¾")
            confidence = probs[0][pred_idx].item()
            return f"è¯†åˆ«ç»“æœ: {label} (ç½®ä¿¡åº¦: {confidence:.2%})"

        except Exception as e:
            return f"é”™è¯¯: {str(e)}"
        finally:
            mp_hands.close()
    def _preprocess_frame(self, frame):
        """é¢„å¤„ç†å•å¸§å›¾åƒ"""
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)
        return transform(pil_image)
    def _process_frame_with_keypoints(self, frame, mp_hands):
        """æå–å•å¸§+å…³é”®ç‚¹"""
        # é¢„å¤„ç†è§†è§‰å¸§
        processed_tensor = self._preprocess_frame(frame)
        
        # æå–å…³é”®ç‚¹
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        hands_results = mp_hands.process(frame_rgb)
        
        # åˆå§‹åŒ–å…³é”®ç‚¹æ•°ç»„ï¼ˆ126ç»´ï¼‰
        keypoints = np.zeros(126, dtype=np.float32)
        
        if hands_results.multi_hand_landmarks:
            hands = hands_results.multi_hand_landmarks[:2]
            for hand_idx, hand in enumerate(hands):
                start = hand_idx * 63
                for lm_idx, landmark in enumerate(hand.landmark[:21]):
                    pos = start + lm_idx * 3
                    if pos + 2 < 126:
                        keypoints[pos] = landmark.x
                        keypoints[pos+1] = landmark.y
                        keypoints[pos+2] = landmark.z
        
        return processed_tensor, torch.tensor(keypoints, dtype=torch.float32)

    def speech_input(self):
        """è¯­éŸ³è¾“å…¥å¤„ç†"""
        try:
            self.btn_voice.setEnabled(False)
            self.btn_voice.setText("æ­£åœ¨ç›‘å¬...")
            self.lbl_status.setText("è¯·å¼€å§‹è¯´è¯ï¼ˆæœ€é•¿8ç§’ï¼‰")
            self.append_history("[è¯­éŸ³] æ­£åœ¨è¯†åˆ«ä¸­...")
            
            self.speech_thread = SpeechThread()
            self.speech_thread.result_signal.connect(self.handle_speech_result)
            self.speech_thread.start()
            
        except Exception as e:
            self.show_error("è¯­éŸ³è¾“å…¥é”™è¯¯", str(e))
            self.reset_voice_button()

    def run_diagnostic_test(self):
        """è¯Šæ–­æµ‹è¯•"""
        # ç”Ÿæˆéšæœºæµ‹è¯•æ•°æ®
        dummy_input = torch.randn(1, 170, 638).to(device)  # æ³¨æ„ç»´åº¦æ”¹ä¸º638
        with torch.no_grad():
            outputs = self.model(dummy_input)
            probs = torch.softmax(outputs, dim=1)
        
        print("\nè¯Šæ–­æµ‹è¯•ç»“æœ:")
        print("Logits:", outputs)
        print("æ¦‚ç‡åˆ†å¸ƒ:", probs)

    def handle_speech_result(self, text):
        """å¤„ç†è¯­éŸ³è¯†åˆ«ç»“æœ"""
        self.append_history(text)
        self.reset_voice_button()

    def reset_voice_button(self):
        """é‡ç½®è¯­éŸ³æŒ‰é’®çŠ¶æ€"""
        self.btn_voice.setEnabled(True)
        self.btn_voice.setText("ğŸ¤ è¯­éŸ³è¾“å…¥")
        self.lbl_status.setText("å°±ç»ª")

    def show_error(self, title, message):
        """æ˜¾ç¤ºé”™è¯¯å¼¹çª—"""
        QMessageBox.critical(self, title, message)
        self.append_history(f"[é”™è¯¯] {title}: {message}")

    def update_preview(self, image):
        """æ›´æ–°é¢„è§ˆç”»é¢"""
        self.preview_label.setPixmap(QPixmap.fromImage(image))

    def append_history(self, text):
        """æ·»åŠ å†å²è®°å½•"""
        from datetime import datetime
        timestamp = datetime.now().strftime("%H:%M:%S")
        cursor = self.history.textCursor()
        cursor.movePosition(QTextCursor.End)
        cursor.insertText(f"[{timestamp}] {text}\n")
        self.history.ensureCursorVisible()

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec_())